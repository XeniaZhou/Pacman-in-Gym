{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“1Rendering OpenAi Gym in Colaboratory.ipynb”的副本",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XeniaZhou/Pacman-in-Gym/blob/master/Rendering_OpenAi_Gym_in_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsSGgH2Y0apl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#          _____                _____                    _____                    _____                    _____                    _____          \n",
        "#         /\\    \\              /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\         \n",
        "#        /::\\    \\            /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\        \n",
        "#       /::::\\    \\           \\:::\\    \\              /::::\\    \\              /::::\\    \\              /::::\\    \\               \\:::\\    \\       \n",
        "#      /::::::\\    \\           \\:::\\    \\            /::::::\\    \\            /::::::\\    \\            /::::::\\    \\               \\:::\\    \\      \n",
        "#     /:::/\\:::\\    \\           \\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\               \\:::\\    \\     \n",
        "#    /:::/__\\:::\\    \\           \\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\               \\:::\\    \\    \n",
        "#    \\:::\\   \\:::\\    \\          /::::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\              /::::\\    \\   \n",
        "#  ___\\:::\\   \\:::\\    \\        /::::::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    ____    /::::::\\    \\  \n",
        "# /\\   \\:::\\   \\:::\\    \\      /:::/\\:::\\    \\  /:::/\\:::\\   \\:::\\    \\  /:::/\\:::\\   \\:::\\____\\  /:::/\\:::\\   \\:::\\    \\  /\\   \\  /:::/\\:::\\    \\ \n",
        "#/::\\   \\:::\\   \\:::\\____\\    /:::/  \\:::\\____\\/:::/  \\:::\\   \\:::\\____\\/:::/  \\:::\\   \\:::|    |/:::/  \\:::\\   \\:::\\____\\/::\\   \\/:::/  \\:::\\____\\\n",
        "#\\:::\\   \\:::\\   \\::/    /   /:::/    \\::/    /\\::/    \\:::\\  /:::/    /\\::/   |::::\\  /:::|____|\\::/    \\:::\\  /:::/    /\\:::\\  /:::/    \\::/    /\n",
        "# \\:::\\   \\:::\\   \\/____/   /:::/    / \\/____/  \\/____/ \\:::\\/:::/    /  \\/____|:::::\\/:::/    /  \\/____/ \\:::\\/:::/    /  \\:::\\/:::/    / \\/____/ \n",
        "#  \\:::\\   \\:::\\    \\      /:::/    /                    \\::::::/    /         |:::::::::/    /            \\::::::/    /    \\::::::/    /          \n",
        "#   \\:::\\   \\:::\\____\\    /:::/    /                      \\::::/    /          |::|\\::::/    /              \\::::/    /      \\::::/____/           \n",
        "#    \\:::\\  /:::/    /    \\::/    /                       /:::/    /           |::| \\::/____/               /:::/    /        \\:::\\    \\           \n",
        "#     \\:::\\/:::/    /      \\/____/                       /:::/    /            |::|  ~|                    /:::/    /          \\:::\\    \\          \n",
        "#      \\::::::/    /                                    /:::/    /             |::|   |                   /:::/    /            \\:::\\    \\         \n",
        "#       \\::::/    /                                    /:::/    /              \\::|   |                  /:::/    /              \\:::\\____\\        \n",
        "#        \\::/    /                                     \\::/    /                \\:|   |                  \\::/    /                \\::/    /        \n",
        "#         \\/____/                                       \\/____/                  \\|___|                   \\/____/                  \\/____/    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "# install dependancies, takes around 45 seconds\n",
        "\n",
        "Rendering Dependancies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A-1LTSH88EE",
        "colab_type": "text"
      },
      "source": [
        "Pacman Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXSx7hg19TH",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD7uYYvbk8vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSFWl7OM__pD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL8iu-_FlRJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3BGbWOu179M",
        "colab_type": "text"
      },
      "source": [
        "# Pacman!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGEFMfDOzLen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"MsPacman-v0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmIlXhe9Q89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check out the pacman action space!\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g8M_ZH_qwKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.contrib.layers import flatten, conv2d, fully_connected\n",
        "from collections import deque, Counter\n",
        "import random\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdklDJXJq257",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "color = np.array([210, 164, 74]).mean()\n",
        "def preprocess_observation(obs):\n",
        "  # Crop and resize the image\n",
        "  img = obs[1:176:2, ::2]\n",
        "  # Convert the image to greyscale\n",
        "  img = img.mean(axis=2)\n",
        "  # Improve image contrast\n",
        "  img[img==color] = 0\n",
        "  # Next we normalize the image from -1 to +1\n",
        "  img =(img-128)/128-1\n",
        "  return img.reshape(88,80,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhx11kOVrHst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_outputs = env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdwt2ORqrezV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def q_network(X, name_scope):\n",
        "# Initialize layers\n",
        "  initializer = tf.keras.initializers.VarianceScaling(scale=2.0)\n",
        "  with tf.variable_scope(name_scope) as scope:\n",
        "    # initialize the convolutional layers\n",
        "    layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4, padding='SAME', weights_initializer=initializer)\n",
        "\n",
        "    layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(4,4),    stride=2, padding='SAME', weights_initializer=initializer)\n",
        "\n",
        "    layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding='SAME', weights_initializer=initializer)\n",
        "\n",
        "    flat = flatten(layer_3)\n",
        "    fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n",
        "   \n",
        "    #Add final output layer\n",
        "    output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)\n",
        "    vars = {v.name[len(scope.name):]: v for v in tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)}\n",
        "    #Return both variables and outputs together\n",
        "  return vars, output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUuwrGSPrzHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_episodes = 800\n",
        "batch_size = 48\n",
        "input_shape = (None, 88, 80, 1)\n",
        "#Recall shape is img.reshape(88,80,1)\n",
        "learning_rate = 0.001\n",
        "X_shape = (None, 88, 80, 1)\n",
        "discount_factor = 0.97\n",
        "global_step = 0\n",
        "copy_steps = 100\n",
        "steps_train = 4\n",
        "start_steps = 2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJVcaL_Gr1sV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon = 0.5\n",
        "eps_min = 0.05\n",
        "eps_max = 1.0\n",
        "eps_decay_steps = 500000\n",
        "#\n",
        "def epsilon_greedy(action, step):\n",
        "  p = np.random.random(1).squeeze() #1D entries returned using squeeze\n",
        "  epsilon = max(eps_min, eps_max -(eps_max-eps_min)*step/eps_decay_steps) #Decaying policy with more steps\n",
        "  if np.random.rand() < epsilon:\n",
        "    return np.random.randint(n_outputs)\n",
        "  else:\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AByNFCbKr9-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "buffer_len = 20000\n",
        "#Buffer is made from a deque — double ended queue\n",
        "exp_buffer = deque(maxlen=buffer_len)\n",
        "def sample_memories(batch_size):\n",
        "  perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]\n",
        "  mem = np.array(exp_buffer)[perm_batch]\n",
        "  return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRlDK2RQsAlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we build our Q network, which takes the input X and generates Q values for all the actions in the state\n",
        "tf.reset_default_graph()\n",
        "X = tf.placeholder(tf.float32, shape=[None, 88, 80,1])    \n",
        "print(X.name)   \n",
        "mainQ, mainQ_outputs = q_network(X, 'mainQ')\n",
        "# similarly we build our target Q network, for policy evaluation\n",
        "targetQ, targetQ_outputs = q_network(X, 'targetQ')\n",
        "copy_op = [tf.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]\n",
        "copy_target_to_main = tf.group(*copy_op)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B35lEa_DtS-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_action = tf.placeholder(tf.int32, shape=[None]) \n",
        "print(X.name)\n",
        "Q_action = tf.reduce_sum(targetQ_outputs*tf.one_hot(X_action, 9), axis=1, keep_dims=True)\n",
        "# define a placeholder for our output i.e action\n",
        "y = tf.placeholder(tf.float32, shape=(None,1))\n",
        "# now we calculate the loss which is the difference between actual value and predicted value\n",
        "loss = tf.reduce_mean(input_tensor=tf.square(y - Q_action))\n",
        "# we use adam optimizer for minimizing the loss\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "init = tf.global_variables_initializer()\n",
        "loss_summary = tf.summary.scalar('LOSS', loss)\n",
        "merge_summary = tf.summary.merge_all()\n",
        "file_writer = tf.summary.FileWriter(\"log.txt\", tf.get_default_graph())\n",
        "X=tf.get_default_graph().get_tensor_by_name(\"Placeholder:0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgKBxPxOvXdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  # for each episode\n",
        "  history = []\n",
        "  for i in range(500):\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    epoch = 0\n",
        "    episodic_reward = 0\n",
        "    actions_counter = Counter()\n",
        "    episodic_loss = []\n",
        "    # while the state is not the terminal state\n",
        "    while not done:\n",
        "    # get the preprocessed game screen\n",
        "      obs = preprocess_observation(obs)\n",
        "    # feed the game screen and get the Q values for each action, \n",
        "      actions = mainQ_outputs.eval(feed_dict={X:[obs]})\n",
        "    # get the action\n",
        "      action = np.argmax(actions, axis=-1)\n",
        "      actions_counter[str(action)] += 1\n",
        "    # select the action using epsilon greedy policy\n",
        "      action = epsilon_greedy(action, global_step)\n",
        "    # now perform the action and move to the next state, next_obs, receive reward\n",
        "      next_obs, reward, done, _ = env.step(action)\n",
        "    # Store this transition as an experience in the replay buffer! Quite important\n",
        "      exp_buffer.append([obs, action, preprocess_observation(next_obs), reward, done])\n",
        "    # After certain steps we move on to generating y-values for Q network with samples from the experience replay buffer\n",
        "      if global_step % steps_train == 0 and global_step > start_steps:\n",
        "        o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)\n",
        "        # states\n",
        "        o_obs = [x for x in o_obs]\n",
        "        # next states\n",
        "        o_next_obs = [x for x in o_next_obs]\n",
        "        # next actions\n",
        "        next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs})\n",
        "        #discounted reward for action: these are our Y-values\n",
        "        y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done)\n",
        "        # merge all summaries and write to the file\n",
        "        mrg_summary = merge_summary.eval(feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act})\n",
        "        file_writer.add_summary(mrg_summary, global_step)\n",
        "        # To calculate the loss, we run the previously defined functions mentioned while feeding inputs\n",
        "        train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act})\n",
        "        episodic_loss.append(train_loss)\n",
        "  # after some interval we copy our main Q network weights to target Q network\n",
        "      if (global_step+1) % copy_steps == 0 and global_step > start_steps:\n",
        "        copy_target_to_main.run()\n",
        "      obs = next_obs\n",
        "      epoch += 1\n",
        "      global_step += 1\n",
        "      episodic_reward += reward\n",
        "    history.append(episodic_reward)\n",
        "    print('Epochs per episode:', epoch, 'Episode Reward:', episodic_reward,\"Episode number:\", i, flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZR_MYYNzA_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluate model on openAi GYM\n",
        "env = wrap_env(env)\n",
        "observation = env.reset()\n",
        "frames=[]\n",
        "new_observation = observation\n",
        "prev_input = None\n",
        "done = False\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  while True:\n",
        "    if True:\n",
        "    #set input to network to be difference image\n",
        "      obs = preprocess_observation(observation)\n",
        "    # feed the game screen and get the Q values for each action\n",
        "      actions = mainQ_outputs.eval(feed_dict={X:[obs]})\n",
        "    # get the action\n",
        "      action = np.argmax(actions, axis=-1)\n",
        "      actions_counter[str(action)] += 1\n",
        "    # select the action using epsilon greedy policy\n",
        "      action = epsilon_greedy(action, global_step)\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      observation = new_observation\n",
        "    # now perform the action and move to the next state, next_obs, receive reward\n",
        "      new_observation, reward, done, _ = env.step(action)\n",
        "      if done:\n",
        "      #observation = env.reset()\n",
        "        break\n",
        "  env.close()\n",
        "  show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT-qSG261EdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    display(display_animation(anim, default_mode='loop'))\n",
        "\n",
        "display_frames_as_gif(frames)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}